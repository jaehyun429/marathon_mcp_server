# marathon_server.py
# type: ignore
from fastmcp import FastMCP
import httpx
from bs4 import BeautifulSoup
import json
import asyncio
from datetime import datetime, timedelta
from typing import Optional
import sys

mcp = FastMCP("marathon-crawler")

# ê°„ë‹¨í•œ ë©”ëª¨ë¦¬ ìºì‹œ
_cache = {
    'data': None,
    'timestamp': None,
    'ttl': 3600  # 1ì‹œê°„ ìºì‹œ
}

async def fetch_detail(client: httpx.AsyncClient, detail_url: str, base_domain: str) -> Optional[dict]:
    """ì •ì  ë‹¨ì¼ ë§ˆë¼í†¤ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
    try:
        full_url = base_domain + detail_url if not detail_url.startswith('http') else detail_url
        response = await client.get(full_url, timeout= 15.0)
        response.raise_for_status()

        html = response.text
        soup = BeautifulSoup(html, 'html.parser')
        
        script_tag = soup.find('script', {'id': '__NEXT_DATA__'})
        
        if script_tag:
            json_data = json.loads(script_tag.string)
            race_detail = json_data.get('props', {}).get('pageProps', {}).get('raceDetail', {})
            
            if race_detail:
                return {
                    'ë§ˆë¼í†¤ëª…': race_detail.get('raceName', ''),
                    'íŠ¸ë™': race_detail.get('raceTypeList', '').split(',') if race_detail.get('raceTypeList') else [],
                    'ì§€ì—­': race_detail.get('region', ''),
                    'ì¥ì†Œ': race_detail.get('place', ''),
                    'ë‚ ì§œ': race_detail.get('raceDate', ''),
                    'ì§‘ê²°ì‹œê°„': race_detail.get('raceStart', ''),
                    'ì ‘ìˆ˜ê¸°ê°„': {
                        'ì‹œì‘ì¼': race_detail.get('applicationStartDate', ''),
                        'ì¢…ë£Œì¼': race_detail.get('applicationEndDate', '')
                    },
                    'ë¬¸ì˜ì²˜': {
                        'ì´ë©”ì¼': race_detail.get('email', ''),
                        'ì „í™”ë²ˆí˜¸': race_detail.get('phone', '')
                    },
                    'ì£¼ìµœ': race_detail.get('host', ''),
                    'í™ˆí˜ì´ì§€': race_detail.get('homepageUrl', ''),
                    'ì†Œê°œ': race_detail.get('intro', ''),
                    'ìƒì„¸URL': detail_url
                }
    except Exception as e:
        print(f"Error fetching {detail_url}: {e}", file=sys.stderr)
        return None

def is_accepting_applications(marathon: dict) -> bool:
    """ì ‘ìˆ˜ ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
    try:
        end_date_str = marathon.get('ì ‘ìˆ˜ê¸°ê°„', {}).get('ì¢…ë£Œì¼', '')
        if not end_date_str:
            return False
        
        end_date = datetime.strptime(end_date_str, '%Y-%m-%d')
        today = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
        
        return end_date >= today
    except:
        return False

def format_marathon_info(marathon: dict, include_contact: bool = True) -> str:
    """ë§ˆë¼í†¤ ì •ë³´ë¥¼ ë³´ê¸° ì¢‹ê²Œ í¬ë§·íŒ…"""
    lines = []
    
    # ê¸°ë³¸ ì •ë³´
    lines.append(f"ğŸƒ {marathon.get('ë§ˆë¼í†¤ëª…', 'ì •ë³´ ì—†ìŒ')}")
    
    # íŠ¸ë™ ì •ë³´
    tracks = marathon.get('íŠ¸ë™', [])
    if tracks:
        tracks_str = ', '.join([t.strip() for t in tracks if t.strip()])
        lines.append(f"ğŸ“ íŠ¸ë™: {tracks_str}")
    
    # ë‚ ì§œ ë° ì¥ì†Œ
    race_date = marathon.get('ë‚ ì§œ', '')
    if race_date:
        lines.append(f"ğŸ“… ë‚ ì§œ: {race_date}")
    
    gathering_time = marathon.get('ì§‘ê²°ì‹œê°„', '')
    if gathering_time:
        lines.append(f"â° {gathering_time}")
    
    location = marathon.get('ì§€ì—­', '')
    place = marathon.get('ì¥ì†Œ', '')
    if location or place:
        loc_str = f"{location} - {place}" if location and place else (location or place)
        lines.append(f"ğŸ“ ì¥ì†Œ: {loc_str}")
    
    # ì ‘ìˆ˜ ê¸°ê°„
    app_period = marathon.get('ì ‘ìˆ˜ê¸°ê°„', {})
    start_date = app_period.get('ì‹œì‘ì¼', '')
    end_date = app_period.get('ì¢…ë£Œì¼', '')
    
    if start_date and end_date:
        is_open = is_accepting_applications(marathon)
        status = "âœ… ì ‘ìˆ˜ ì¤‘" if is_open else "âŒ ì ‘ìˆ˜ ë§ˆê°"
        lines.append(f"ğŸ“ ì ‘ìˆ˜ê¸°ê°„: {start_date} ~ {end_date} ({status})")
    elif end_date:
        is_open = is_accepting_applications(marathon)
        status = "âœ… ì ‘ìˆ˜ ì¤‘" if is_open else "âŒ ì ‘ìˆ˜ ë§ˆê°"
        lines.append(f"ğŸ“ ì ‘ìˆ˜ ë§ˆê°: {end_date} ({status})")
    
    # ë¬¸ì˜ì²˜ (ìš”ì²­ ì‹œì—ë§Œ)
    if include_contact:
        contact = marathon.get('ë¬¸ì˜ì²˜', {})
        email = contact.get('ì´ë©”ì¼', '')
        phone = contact.get('ì „í™”ë²ˆí˜¸', '')
        
        if email or phone:
            lines.append("ğŸ“ ë¬¸ì˜ì²˜:")
            if email:
                lines.append(f"   âœ‰ï¸ {email}")
            if phone:
                lines.append(f"   ğŸ“± {phone}")
    
    # ì£¼ìµœ
    host = marathon.get('ì£¼ìµœ', '')
    if host:
        lines.append(f"ğŸ¢ ì£¼ìµœ: {host}")
    
    # í™ˆí˜ì´ì§€
    homepage = marathon.get('í™ˆí˜ì´ì§€', '')
    if homepage:
        lines.append(f"ğŸ”— {homepage}")
    
    # ì†Œê°œ
    intro = marathon.get('ì†Œê°œ', '')
    if intro and len(intro) > 10:
        intro_short = intro[:100] + '...' if len(intro) > 100 else intro
        lines.append(f"â„¹ï¸ {intro_short}")
    
    return '\n'.join(lines)

async def crawl_marathons_fast(base_url: str, base_domain: str, max_concurrent: int = 10) -> list:
    """ë³‘ë ¬ ì²˜ë¦¬ë¡œ ë¹ ë¥´ê²Œ í¬ë¡¤ë§"""
    all_marathons = []
    
    async with httpx.AsyncClient() as client:
        try:
            # 1. ëª©ë¡ í˜ì´ì§€ HTML ì •ì  ìš”ì²­
            response = await client.get(base_url, timeout=30.0)
            response.raise_for_status()
            html = response.text
            
            soup = BeautifulSoup(html, 'html.parser')
            marathon_links = soup.find_all('a', class_='MuiLink-root')
            
            detail_urls = []
            for link in marathon_links:
                href = link.get('href', '')
                if href and '/raceDetail/' in href and href not in detail_urls:
                    detail_urls.append(href)
            
            if not detail_urls:
                print("ê²½ê³ : ìƒì„¸ í˜ì´ì§€ ë§í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (ì‚¬ì´íŠ¸ êµ¬ì¡° ë³€ê²½ ê°€ëŠ¥ì„±)", file=sys.stderr)
                return []
            
            # 2. ë³‘ë ¬ë¡œ ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§
            semaphore = asyncio.Semaphore(max_concurrent)
            
            async def fetch_with_semaphore(url):
                async with semaphore:
                    result = await fetch_detail(client, url, base_domain)
                    return result
            
            tasks = [fetch_with_semaphore(url) for url in detail_urls]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            all_marathons = [r for r in results if r and not isinstance(r, Exception)]
            
        except httpx.HTTPStatusError as e:
            print(f"HTTP ì˜¤ë¥˜ ë°œìƒ : {e.response.status_code} - {e.request.url}", file=sys.stderr)
        except Exception as e:
            print(f"í¬ë¡¤ë§ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ë°œìƒ : {e}", file=sys.stderr)
    
    return all_marathons

def is_cache_valid() -> bool:
    """ìºì‹œê°€ ìœ íš¨í•œì§€ í™•ì¸"""
    if _cache['data'] is None or _cache['timestamp'] is None:
        return False
    
    elapsed = datetime.now() - _cache['timestamp']
    return elapsed.total_seconds() < _cache['ttl']

@mcp.tool()
async def crawl_korean_marathons(
    base_url: str = "https://marathongo.co.kr/races",
    base_domain: str = "https://marathongo.co.kr",
    region_filter: str = "",
    date_filter: str = "",
    only_accepting: bool = False,
    use_cache: bool = True
) -> str:
    """
    í•œêµ­ì˜ ë§ˆë¼í†¤ ëŒ€íšŒ ì •ë³´ë¥¼ í¬ë¡¤ë§í•˜ì—¬ ìƒì„¸ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    
    ë³‘ë ¬ ì²˜ë¦¬ë¡œ ë¹ ë¥´ê²Œ í¬ë¡¤ë§í•˜ë©°, 1ì‹œê°„ ë™ì•ˆ ê²°ê³¼ë¥¼ ìºì‹±í•©ë‹ˆë‹¤.
    
    Args:
        base_url: í¬ë¡¤ë§í•  ë§ˆë¼í†¤ ëª©ë¡ í˜ì´ì§€ì˜ URL (ê¸°ë³¸ê°’: ë§ˆë¼í†¤GO)
        base_domain: ì›¹ì‚¬ì´íŠ¸ì˜ ê¸°ë³¸ ë„ë©”ì¸ URL
        region_filter: íŠ¹ì • ì§€ì—­ í•„í„°ë§ (ì˜ˆ: 'ì„œìš¸', 'ê²½ê¸°', 'ë¶€ì‚°'). ë¹„ì›Œë‘ë©´ ì „ì²´ ê²€ìƒ‰
        date_filter: íŠ¹ì • ë‚ ì§œ/ì›” í•„í„°ë§ (ì˜ˆ: '2025-11', '2025-11-15'). ë¹„ì›Œë‘ë©´ ì „ì²´ ê²€ìƒ‰
        only_accepting: Trueì¼ ê²½ìš° í˜„ì¬ ì ‘ìˆ˜ ê°€ëŠ¥í•œ ëŒ€íšŒë§Œ ë°˜í™˜ (ê¸°ë³¸ê°’: False)
        use_cache: ìºì‹œ ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
    
    Returns:
        í¬ë§·íŒ…ëœ ë§ˆë¼í†¤ ì •ë³´ ëª©ë¡
        
    ìˆ˜ì§‘ ì •ë³´:
        - ë§ˆë¼í†¤ëª…, íŠ¸ë™ ì¢…ë¥˜ (10km, 5km ë“±)
        - ê°œìµœ ì§€ì—­ ë° ì¥ì†Œ, ëŒ€íšŒ ë‚ ì§œ, ì§‘ê²° ì‹œê°„
        - ì ‘ìˆ˜ ê¸°ê°„ (ì‹œì‘ì¼, ì¢…ë£Œì¼) ë° ì ‘ìˆ˜ ê°€ëŠ¥ ì—¬ë¶€
        - ë¬¸ì˜ì²˜ (ì´ë©”ì¼, ì „í™”ë²ˆí˜¸)
        - ì£¼ìµœ ê¸°ê´€, í™ˆí˜ì´ì§€, ëŒ€íšŒ ì†Œê°œ
    
    ì‚¬ìš© ì˜ˆì‹œ:
        - "2025ë…„ 11ì›”ì— ìˆëŠ” ë§ˆë¼í†¤ ì•Œë ¤ì¤˜"
        - "ì„œìš¸ì—ì„œ í•˜ëŠ” ë§ˆë¼í†¤ ì°¾ì•„ì¤˜"
        - "ì§€ê¸ˆ ì‹ ì²­í•  ìˆ˜ ìˆëŠ” ë§ˆë¼í†¤ ìˆì–´?"
        - "ì´ë²ˆ ì£¼ë§ ë§ˆë¼í†¤ ëŒ€íšŒ ìˆì–´?"
    """
    
    # ìºì‹œ í™•ì¸
    if use_cache and is_cache_valid():
        results = _cache['data']
    else:
        print("Fetching new data", file=sys.stderr)
        results = await crawl_marathons_fast(base_url, base_domain, max_concurrent=10)
        if results:
            _cache['data'] = results
            _cache['timestamp'] = datetime.now()
            print(f"ë°ì´í„° {len(results)}ê°œ ë¡œë“œ ë° ìºì‹œ ì €ì¥", file=sys.stderr)
        else:
            print("Data fetch failed", file=sys.stderr)
        
    # í•„í„°ë§ ì ìš©
    filtered_results = results
    
    if region_filter:
        filtered_results = [m for m in filtered_results if region_filter in m.get('ì§€ì—­', '')]
    
    if date_filter:
        filtered_results = [m for m in filtered_results if date_filter in m.get('ë‚ ì§œ', '')]
    
    if only_accepting:
        filtered_results = [m for m in filtered_results if is_accepting_applications(m)]
    
    # ë‚ ì§œìˆœ ì •ë ¬
    filtered_results.sort(key=lambda x: x.get('ë‚ ì§œ', '9999-99-99'))
    
    # ê²°ê³¼ í¬ë§·íŒ…
    if not filtered_results:
        message = "í˜„ì¬ ì ‘ìˆ˜ ê°€ëŠ¥í•œ ë§ˆë¼í†¤ì´ ì—†ìŠµë‹ˆë‹¤." if only_accepting else "ê²€ìƒ‰ ì¡°ê±´ì— ë§ëŠ” ë§ˆë¼í†¤ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
        return f"âŒ {message}\n\nğŸ’¡ íŒ: ë‹¤ë¥¸ ì§€ì—­ì´ë‚˜ ë‚ ì§œë¡œ ê²€ìƒ‰í•´ë³´ì„¸ìš”."
    
    # ë§ˆë¼í†¤ ì •ë³´ í¬ë§·íŒ…
    formatted_list = []
    for i, marathon in enumerate(filtered_results, 1):
        formatted = format_marathon_info(marathon, include_contact=True)
        formatted_list.append(f"\n{'='*50}\n[{i}] {formatted}\n{'='*50}")
    
    header = f"âœ… ì´ {len(filtered_results)}ê°œì˜ ë§ˆë¼í†¤ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤"
    if only_accepting:
        header += " (ì ‘ìˆ˜ ê°€ëŠ¥í•œ ëŒ€íšŒë§Œ)"
    header += "\n"
    
    footer = "\n\nğŸ’¡ íŠ¹ì • ë§ˆë¼í†¤ì˜ ìƒì„¸ ì •ë³´ê°€ í•„ìš”í•˜ì‹œë©´ ë§ì”€í•´ì£¼ì„¸ìš”!"
    
    return header + '\n'.join(formatted_list) + footer


@mcp.tool()
async def clear_marathon_cache() -> str:
    """
    ë§ˆë¼í†¤ ë°ì´í„° ìºì‹œë¥¼ ì‚­ì œí•©ë‹ˆë‹¤.
    ìµœì‹  ì •ë³´ê°€ í•„ìš”í•  ë•Œ ì‚¬ìš©í•˜ì„¸ìš”.
    
    Returns:
        ìºì‹œ ì‚­ì œ ê²°ê³¼ ë©”ì‹œì§€
    """
    _cache['data'] = None
    _cache['timestamp'] = None
    
    return "âœ… ìºì‹œê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ìŒ ê²€ìƒ‰ ì‹œ ìµœì‹  ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."


if __name__ == "__main__":
    mcp.run()
